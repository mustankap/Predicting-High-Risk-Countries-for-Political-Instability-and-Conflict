{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xchnxAsaKKqO"
   },
   "source": [
    "## Use the right version of TensorFlow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDWhxaj2OMSv"
   },
   "source": [
    "The following hidden code cell ensures that the Colab will run on TensorFlow 2.X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "form",
    "id": "qBpGi_GD14-p"
   },
   "outputs": [],
   "source": [
    "# @title Run on TensorFlow 2.x\n",
    "# %tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iuw6-JOGf7I"
   },
   "source": [
    "## Call the import statements\n",
    "\n",
    "The following code imports the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "form",
    "id": "9n9_cTveKmse"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran the import statements.\n"
     ]
    }
   ],
   "source": [
    "#@title Load the imports\n",
    "\n",
    "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "# tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "print(\"Ran the import statements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_TaJhU4KcuY"
   },
   "source": [
    "## Load the datasets \n",
    "\n",
    "The following code cell loads the separate .csv files and creates the following two pandas DataFrames:\n",
    "\n",
    "* `train_df`, which contains the training set\n",
    "* `test_df`, which contains the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "JZlvdpyYKx7V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                   object\n",
       "Year                       int64\n",
       "Rank                      object\n",
       "Total                    float64\n",
       "Security_Apparatus       float64\n",
       "                          ...   \n",
       "Public_Services          float64\n",
       "Human_Rights             float64\n",
       "Demographic_Pressures    float64\n",
       "Refugees_and_IDPs        float64\n",
       "External_Intervention    float64\n",
       "Length: 16, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"Dataset/training/fsi-2006-to-2013.csv\")\n",
    "test_df = pd.read_csv(\"Dataset/testing/fsi-2014-to2020.csv\")\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "n7nuAHoZIgVI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                   object\n",
       "Year                       int64\n",
       "Rank                      object\n",
       "Total                    float64\n",
       "Security_Apparatus       float64\n",
       "                          ...   \n",
       "Public_Services          float64\n",
       "Human_Rights             float64\n",
       "Demographic_Pressures    float64\n",
       "Refugees_and_IDPs        float64\n",
       "External_Intervention    float64\n",
       "Length: 16, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Total'] = train_df['Total']/10\n",
    "train_df_mean = train_df.mean()\n",
    "train_df_std = train_df.std()\n",
    "train_df_norm = (train_df - train_df_mean)/train_df_std\n",
    "train_df_norm.head()\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "QoW-59jVFF2I"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Change_from_Previous_Year</th>\n",
       "      <th>Country</th>\n",
       "      <th>Demographic_Pressures</th>\n",
       "      <th>Economic_Inequality</th>\n",
       "      <th>Economy</th>\n",
       "      <th>External_Intervention</th>\n",
       "      <th>Factionalized_Elites</th>\n",
       "      <th>Group_Grievance</th>\n",
       "      <th>Human_Flight_and_Brain_Drain</th>\n",
       "      <th>Human_Rights</th>\n",
       "      <th>Public_Services</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Refugees_and_IDPs</th>\n",
       "      <th>Security_Apparatus</th>\n",
       "      <th>State_Legitimacy</th>\n",
       "      <th>Total</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Change_from_Previous_Year Country  Demographic_Pressures  \\\n",
       "0                        0.2     NaN                    1.7   \n",
       "1                        0.0     NaN                    1.7   \n",
       "2                       -0.0     NaN                    1.5   \n",
       "3                        0.4     NaN                    0.8   \n",
       "4                        0.4     NaN                    1.7   \n",
       "\n",
       "   Economic_Inequality  Economy  External_Intervention  Factionalized_Elites  \\\n",
       "0                  0.9      1.9                    1.7                   1.5   \n",
       "1                  1.7      1.8                    1.3                   1.5   \n",
       "2                  1.6      2.0                    1.5                   1.3   \n",
       "3                  0.7      1.6                    1.7                   1.4   \n",
       "4                  1.3      1.2                    1.5                   1.4   \n",
       "\n",
       "   Group_Grievance  Human_Flight_and_Brain_Drain  Human_Rights  \\\n",
       "0              1.7                           0.7           1.7   \n",
       "1              1.2                           1.6           1.3   \n",
       "2              1.4                           0.6           1.3   \n",
       "3              1.9                           1.4           1.7   \n",
       "4              1.7                           0.6           1.5   \n",
       "\n",
       "   Public_Services Rank  Refugees_and_IDPs  Security_Apparatus  \\\n",
       "0              1.6  NaN                1.9                 1.8   \n",
       "1              1.4  NaN                1.7                 1.8   \n",
       "2              1.6  NaN                1.9                 1.6   \n",
       "3              1.4  NaN                2.0                 1.8   \n",
       "4              1.6  NaN                2.0                 1.2   \n",
       "\n",
       "   State_Legitimacy  Total  Year  \n",
       "0               1.5    1.8   1.5  \n",
       "1               1.1    1.7   1.5  \n",
       "2               1.5    1.7   1.5  \n",
       "3               1.5    1.7   1.5  \n",
       "4               1.4    1.7   1.5  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_df['Total'] = test_df['Total']/10\n",
    "test_df_mean = test_df.mean()\n",
    "test_df_std  = test_df.std()\n",
    "test_df_norm = (test_df - test_df_mean)/test_df_std\n",
    "test_df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Total</th>\n",
       "      <th>Security_Apparatus</th>\n",
       "      <th>Factionalized_Elites</th>\n",
       "      <th>Group_Grievance</th>\n",
       "      <th>Economy</th>\n",
       "      <th>Economic_Inequality</th>\n",
       "      <th>Human_Flight_and_Brain_Drain</th>\n",
       "      <th>State_Legitimacy</th>\n",
       "      <th>Public_Services</th>\n",
       "      <th>Human_Rights</th>\n",
       "      <th>Demographic_Pressures</th>\n",
       "      <th>Refugees_and_IDPs</th>\n",
       "      <th>External_Intervention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>1387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2009.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.4</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2006.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2008.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>7.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>7.8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Year  Total  Security_Apparatus  Factionalized_Elites  \\\n",
       "count 1387.0 1387.0              1387.0                1387.0   \n",
       "mean  2009.6    7.1                 5.7                   6.1   \n",
       "std      2.3    2.3                 2.4                   2.5   \n",
       "min   2006.0    1.7                 0.9                   0.7   \n",
       "25%   2008.0    5.6                 4.1                   4.2   \n",
       "50%   2010.0    7.7                 6.0                   6.8   \n",
       "75%   2012.0    8.8                 7.5                   8.0   \n",
       "max   2013.0   11.5                10.0                  10.0   \n",
       "\n",
       "       Group_Grievance  Economy  Economic_Inequality  \\\n",
       "count           1387.0   1387.0               1387.0   \n",
       "mean               6.0      5.8                  6.7   \n",
       "std                2.0      1.9                  1.8   \n",
       "min                1.0      1.0                  1.0   \n",
       "25%                4.5      4.3                  5.7   \n",
       "50%                6.0      5.9                  7.1   \n",
       "75%                7.4      7.3                  8.0   \n",
       "max               10.0     10.0                  9.7   \n",
       "\n",
       "       Human_Flight_and_Brain_Drain  State_Legitimacy  Public_Services  \\\n",
       "count                        1387.0            1387.0           1387.0   \n",
       "mean                            5.6               6.4              5.8   \n",
       "std                             2.0               2.4              2.3   \n",
       "min                             1.0               0.5              1.0   \n",
       "25%                             4.3               5.1              4.0   \n",
       "50%                             6.0               7.0              6.0   \n",
       "75%                             7.1               8.1              7.8   \n",
       "max                            10.0              10.0             10.0   \n",
       "\n",
       "       Human_Rights  Demographic_Pressures  Refugees_and_IDPs  \\\n",
       "count        1387.0                 1387.0             1387.0   \n",
       "mean            5.9                    6.3                5.1   \n",
       "std             2.3                    2.1                2.3   \n",
       "min             1.0                    0.8                0.9   \n",
       "25%             4.3                    4.9                3.2   \n",
       "50%             6.3                    6.5                5.0   \n",
       "75%             7.6                    8.0                6.8   \n",
       "max            10.0                   10.0               10.0   \n",
       "\n",
       "       External_Intervention  \n",
       "count                 1387.0  \n",
       "mean                     5.8  \n",
       "std                      2.2  \n",
       "min                      0.8  \n",
       "25%                      4.4  \n",
       "50%                      6.1  \n",
       "75%                      7.3  \n",
       "max                     10.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df.corr()\n",
    "# train_df.info()\n",
    "train_df.describe( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "b8b2fNmHO-iU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      1.0\n",
       "2      1.0\n",
       "3      1.0\n",
       "4      1.0\n",
       "        ..\n",
       "1382   0.0\n",
       "1383   0.0\n",
       "1384   0.0\n",
       "1385   0.0\n",
       "1386   0.0\n",
       "Name: median_house_value_is_high, Length: 1387, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Double-click for possible solutions.\n",
    "\n",
    "# We arbitrarily set the threshold to 265,000, which is \n",
    "# the 75th percentile for median house values.  Every neighborhood\n",
    "# with a median house price above 265,000 will be labeled 1, \n",
    "# and all other neighborhoods will be labeled 0.\n",
    "threshold = 9\n",
    "train_df_norm[\"median_house_value_is_high\"] = (train_df[\"Total\"] > threshold).astype(float)\n",
    "test_df_norm[\"median_house_value_is_high\"] = (test_df[\"Total\"] > threshold).astype(float) \n",
    "train_df_norm[\"median_house_value_is_high\"].head(8000)\n",
    "\n",
    "\n",
    "# Alternatively, instead of picking the threshold\n",
    "# based on raw house values, you can work with Z-scores.\n",
    "# For example, the following possible solution uses a Z-score\n",
    "# of +1.0 as the threshold, meaning that no more\n",
    "# than 16% of the values in median_house_value_is_high\n",
    "# will be labeled 1.\n",
    "\n",
    "# threshold_in_Z = 1.0 \n",
    "# train_df_norm[\"median_house_value_is_high\"] = (train_df_norm[\"median_house_value\"] > threshold_in_Z).astype(float)\n",
    "# test_df_norm[\"median_house_value_is_high\"] = (test_df_norm[\"median_house_value\"] > threshold_in_Z).astype(float) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kir8UTUXSV8"
   },
   "source": [
    "## Represent features in feature columns\n",
    "\n",
    "This code cell specifies the features that you'll ultimately train the model on and how each of those features will be represented. The transformations (collected in `feature_layer`) don't actually get applied until you pass a DataFrame to it, which will happen when we train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "3tmmZIDw4JEC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1387, 12), dtype=float32, numpy=\n",
       "array([[ 1.5307974,  0.9408891,  1.878555 , ...,  2.1330411,  1.6568434,\n",
       "         1.2987458],\n",
       "       [ 1.7723134,  1.159695 ,  1.4147588, ...,  2.1330411,  1.7807449,\n",
       "         1.340441 ],\n",
       "       [ 1.1926749,  0.9955906,  1.0540284, ...,  2.1330411,  1.698144 ,\n",
       "         1.340441 ],\n",
       "       ...,\n",
       "       [-1.6089113, -2.5600057, -1.8318151, ..., -1.533415 , -1.9362992,\n",
       "        -2.2453513],\n",
       "       [-1.6089113, -2.5600057, -2.3471444, ..., -1.1020671, -1.9362992,\n",
       "        -2.2453513],\n",
       "       [-1.6089113, -2.5600057, -2.0379467, ..., -1.533415 , -1.9362992,\n",
       "        -2.2453513]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty list that will eventually hold all created feature columns.\n",
    "feature_columns = []\n",
    "\n",
    "# Create a numerical feature column to represent median_income.\n",
    "all_features = ['Security_Apparatus','Factionalized_Elites','Group_Grievance','Economy',\n",
    "                'Economic_Inequality','Human_Flight_and_Brain_Drain','State_Legitimacy',\n",
    "                'Public_Services','Human_Rights','Demographic_Pressures','Refugees_and_IDPs',\n",
    "                'External_Intervention']\n",
    "for f in all_features:\n",
    "  x=tf.feature_column.numeric_column(f)\n",
    "  feature_columns.append(x)\n",
    "# median_income = tf.feature_column.numeric_column(\"Security_Apparatus\")\n",
    "# feature_columns.append(median_income)\n",
    "\n",
    "# # Create a numerical feature column to represent total_rooms.\n",
    "# tr = tf.feature_column.numeric_column(\"Factionalized_Elites\")\n",
    "# feature_columns.append(tr)\n",
    "\n",
    "# Convert the list of feature columns into a layer that will later be fed into\n",
    "# the model. \n",
    "feature_layer = layers.DenseFeatures(feature_columns)\n",
    "\n",
    "# Print the first 3 and last 3 rows of the feature_layer's output when applied\n",
    "# to train_df_norm:\n",
    "feature_layer(dict(train_df_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3014ezH3C7jT"
   },
   "source": [
    "## Define functions that build and train a model\n",
    "\n",
    "The following code cell defines two functions:\n",
    "\n",
    "  * `create_model(my_learning_rate, feature_layer, my_metrics)`, which defines the model's\n",
    "    topography.\n",
    "  * `train_model(model, dataset, epochs, label_name, batch_size, shuffle)`, uses input features and labels to train the model.\n",
    "\n",
    "Prior exercises used [ReLU](https://developers.google.com/machine-learning/glossary#ReLU) as the [activation function](https://developers.google.com/machine-learning/glossary#activation_function). By contrast, this exercise uses [sigmoid](https://developers.google.com/machine-learning/glossary#sigmoid_function) as the activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "pedD5GhlDC-y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the create_model and train_model functions.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the functions that create and train a model.\n",
    "def create_model(my_learning_rate, feature_layer, my_metrics):\n",
    "  \"\"\"Create and compile a simple classification model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # Add the feature layer (the list of features and how they are represented)\n",
    "  # to the model.\n",
    "  model.add(feature_layer)\n",
    "\n",
    "  # Funnel the regression value through a sigmoid function.\n",
    "  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
    "                                  activation=tf.sigmoid),)\n",
    "\n",
    "  # Call the compile method to construct the layers into a model that\n",
    "  # TensorFlow can execute.  Notice that we're using a different loss\n",
    "  # function for classification than for regression.    \n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),                                                   \n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=my_metrics)\n",
    "\n",
    "  return model        \n",
    "\n",
    "\n",
    "def train_model(model, dataset, epochs, label_name,\n",
    "                batch_size=None, shuffle=True):\n",
    "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
    "\n",
    "  # The x parameter of tf.keras.Model.fit can be a list of arrays, where\n",
    "  # each array contains the data for one feature.  Here, we're passing\n",
    "  # every column in the dataset. Note that the feature_layer will filter\n",
    "  # away most of those columns, leaving only the desired columns and their\n",
    "  # representations as features.\n",
    "  features = {name:np.array(value) for name, value in dataset.items()}\n",
    "  label = np.array(features.pop(label_name)).astype('float64')\n",
    "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=shuffle)\n",
    "  \n",
    "  # The list of epochs is stored separately from the rest of history.\n",
    "  epochs = history.epoch\n",
    "\n",
    "  # Isolate the classification metric for each epoch.\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  return epochs, hist  \n",
    "\n",
    "print(\"Defined the create_model and train_model functions.\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak_TMAzGOIFq"
   },
   "source": [
    "## Define a plotting function\n",
    "\n",
    "The following [matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) function plots one or more curves, showing how various classification metrics change with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "QF0BFRXTOeR3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_curve function.\n"
     ]
    }
   ],
   "source": [
    "#@title Define the plotting function.\n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "  # list_of_metrics should be one of the names shown in:\n",
    "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Value\")\n",
    "\n",
    "  for m in list_of_metrics:\n",
    "    x = hist[m]\n",
    "    plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "print(\"Defined the plot_curve function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-IXYVfvM4gD"
   },
   "source": [
    "## Invoke the creating, training, and plotting functions\n",
    "\n",
    "The following code cell calls specify the hyperparameters, and then invokes the \n",
    "functions to create and train the model, and then to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cellView": "both",
    "id": "nj3v5EKQFY8s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vedant/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-5bdff748f030>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Train the model on the training set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m epochs, hist = train_model(my_model, train_df_norm, epochs, \n\u001b[0m\u001b[1;32m     21\u001b[0m                            label_name, batch_size)\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-ba863d24cffe>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, epochs, label_name, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   history = model.fit(x=features, y=label, batch_size=batch_size,\n\u001b[0m\u001b[1;32m     37\u001b[0m                       epochs=epochs, shuffle=shuffle)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1131\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1134\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_data_adapter_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1155\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m                **kwargs):\n\u001b[1;32m    246\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    249\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m   \"\"\"\n\u001b[0;32m-> 1430\u001b[0;31m   return convert_to_tensor_v2(\n\u001b[0m\u001b[1;32m   1431\u001b[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   \u001b[0;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m   return convert_to_tensor(\n\u001b[0m\u001b[1;32m   1437\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[0;32m--> 264\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    265\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "from time import time\n",
    "st=time()\n",
    "learning_rate = 0.001\n",
    "epochs = 2000\n",
    "batch_size = 50\n",
    "label_name = \"median_house_value_is_high\"\n",
    "classification_threshold = 0.4\n",
    "\n",
    "# Establish the metrics the model will measure.\n",
    "METRICS = [\n",
    "           tf.keras.metrics.BinaryAccuracy(name='accuracy', \n",
    "                                           threshold=classification_threshold),\n",
    "          ]\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, feature_layer, METRICS)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(my_model, train_df_norm, epochs, \n",
    "                           label_name, batch_size)\n",
    "\n",
    "# Plot a graph of the metric(s) vs. epochs.\n",
    "list_of_metrics_to_plot = ['accuracy'] \n",
    "\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
    "print('it took',time()-st,'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF64TpqkbOpn"
   },
   "source": [
    "Accuracy should gradually improve during training (until it can \n",
    "improve no more)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xNqWWos_zyk"
   },
   "source": [
    "## Evaluate the model against the test set\n",
    "\n",
    "At the end of model training, you ended up with a certain accuracy against the *training set*. Invoke the following code cell to determine your model's accuracy against the *test set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJorkMlDmtHf"
   },
   "outputs": [],
   "source": [
    "features = {name:np.array(value) for name, value in test_df_norm.items()}\n",
    "label = np.array(features.pop(label_name))\n",
    "\n",
    "my_model.evaluate(x = features, y = label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8crSCCVf6gm"
   },
   "source": [
    "## Add precision and recall as metrics\n",
    "\n",
    "Relying solely on accuracy, particularly for a class-imbalanced data set (like ours), can be a poor way to judge a classification model.  Modify the code in the following code cell to enable the model to measure not only accuracy but also precision and recall. We have\n",
    "added accuracy and precision; your task is to add recall. See the [TensorFlow Reference](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall) for details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax87gOyDBhAu"
   },
   "outputs": [],
   "source": [
    "#@title Double-click to view the solution for Task 3.\n",
    "\n",
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 2000\n",
    "batch_size = 50\n",
    "label_name = \"median_house_value_is_high\"\n",
    "classification_threshold = 0.4\n",
    "\n",
    "# Here is the updated definition of METRICS:\n",
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy', \n",
    "                                      threshold=classification_threshold),\n",
    "      tf.keras.metrics.Precision(thresholds=classification_threshold,\n",
    "                                 name='precision' \n",
    "                                 ),\n",
    "      tf.keras.metrics.Recall(thresholds=classification_threshold,\n",
    "                              name=\"recall\"),\n",
    "]\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, feature_layer, METRICS)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(my_model, train_df_norm, epochs, \n",
    "                           label_name, batch_size)\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['accuracy', \"precision\", \"recall\"] \n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
    "print('it took',time()-st,'sec')\n",
    "\n",
    "# The new graphs suggest that precision and recall are \n",
    "# somewhat in conflict. That is, improvements to one of\n",
    "# those metrics may hurt the other metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBGRS0Ndduus"
   },
   "source": [
    "## Summarize model performance \n",
    "\n",
    "If time permits, add one more metric that attempts to summarize the model's overall performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwNE6syoFvWe"
   },
   "outputs": [],
   "source": [
    "#@title Double-click to view the solution for Task 5.\n",
    "\n",
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.001\n",
    "epochs = 2000\n",
    "batch_size = 50\n",
    "label_name = \"median_house_value_is_high\"\n",
    "classification_threshold = 0.4\n",
    "\n",
    "# AUC is a reasonable \"summary\" metric for \n",
    "# classification models.\n",
    "# Here is the updated definition of METRICS to \n",
    "# measure AUC:\n",
    "METRICS = [\n",
    "      tf.keras.metrics.AUC(num_thresholds=100, name='auc'),\n",
    "]\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model(learning_rate, feature_layer, METRICS)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, hist = train_model(my_model, train_df_norm, epochs, \n",
    "                           label_name, batch_size)\n",
    "\n",
    "# Plot metrics vs. epochs\n",
    "list_of_metrics_to_plot = ['auc'] \n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Binary Classification.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
